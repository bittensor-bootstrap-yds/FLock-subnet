# The MIT License (MIT)

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the “Software”), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of
# the Software.

# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

import os
import argparse
import asyncio
import torch
import typing
import bittensor as bt
import math
import numpy as np
from FLockDataset import constants
from FLockDataset.utils.chain import assert_registered, read_chain_commitment
from FLockDataset.validator.chain import (
    retrieve_model_metadata,
    set_weights_with_err_msg,
)
from FLockDataset.validator.validator_utils import compute_score
from FLockDataset.validator.trainer import (
    train_lora,
    download_dataset,
    clean_cache_folder,
)
from FLockDataset.validator.database import ScoreDB


class Validator:
    @staticmethod
    def config():
        bt.logging.info("Parsing command line arguments")
        parser = argparse.ArgumentParser()
        parser.add_argument(
            "--blocks_per_epoch",
            type=int,
            default=360,
            help="Number of blocks to wait before setting weights.",
        )
        parser.add_argument(
            "--miner_sample_size",
            type=int,
            default=3,
            help="Number of miners to sample for each block.",
        )
        parser.add_argument("--netuid", type=int, required=True, help="The subnet UID.")


        parser.add_argument(
            "--cache_dir",
            type=str,
            default="~/data/hf_cache",
            help="Directory to store downloaded model files.",
        )

        parser.add_argument(
            "--data_dir",
            type=str,
            default="~/data/training_data",
            help="Directory to store miner datasets.",
        )

        parser.add_argument(
            "--eval_data_dir",
            type=str,
            default="~/data/eval_data",
            help="Directory to store evaluation datasets.",
        )

        bt.subtensor.add_args(parser)
        bt.logging.add_args(parser)
        bt.wallet.add_args(parser)
        bt.axon.add_args(parser)
        config = bt.config(parser)
        bt.logging.debug(f"Parsed config: {config}")
        return config

    def __init__(self):
        bt.logging.info("Initializing validator")
        self.config = Validator.config()

        if self.config.cache_dir and self.config.cache_dir.startswith('~'):
            self.config.cache_dir = os.path.expanduser(self.config.cache_dir)
        
        if self.config.data_dir and self.config.data_dir.startswith('~'):
            self.config.data_dir = os.path.expanduser(self.config.data_dir)
        
        if self.config.eval_data_dir and self.config.eval_data_dir.startswith('~'):
            self.config.eval_data_dir = os.path.expanduser(self.config.eval_data_dir)

        bt.logging(config=self.config)
        bt.logging.info(f"Starting validator with config: {self.config}")

        # === Bittensor objects ====
        bt.logging.info("Initializing wallet")
        self.wallet = bt.wallet(config=self.config)
        bt.logging.info(f"Wallet initialized: {self.wallet}")
        bt.logging.info("Initializing subtensor")
        try:
            self.subtensor = bt.subtensor(config=self.config)
            bt.logging.info(f"Subtensor initialized: {self.subtensor}")
            bt.logging.info(f"Connected to network: {self.subtensor.network}")
            bt.logging.info(f"Chain endpoint: {self.subtensor.chain_endpoint}")
        except Exception as e:
            bt.logging.error(f"Failed to initialize subtensor: {e}")
            raise

        self.dendrite = bt.dendrite(wallet=self.wallet)

        bt.logging.info(f"Fetching metagraph for netuid: {self.config.netuid}")
        self.metagraph: bt.metagraph = self.subtensor.metagraph(self.config.netuid)
        torch.backends.cudnn.benchmark = True

        bt.logging.info("Checking if wallet is registered on subnet")
        self.uid = assert_registered(self.wallet, self.metagraph)

        bt.logging.info("Initializing weights tensor")
        self.weights = torch.zeros_like(torch.tensor(self.metagraph.S))
        bt.logging.info(f"Weights initialized with shape: {self.weights.shape}")

        self.uids_to_eval: typing.Dict[str, typing.List] = {}
        bt.logging.info("Initializing score database")
        self.score_db = ScoreDB("scores.db")
        bt.logging.info("Score database initialized")
        self.rng = np.random.default_rng()
        bt.logging.info("Validator initialization complete")

    async def try_sync_metagraph(self) -> bool:
        bt.logging.trace("Syncing metagraph")
        try:
            self.metagraph = self.subtensor.metagraph(self.config.netuid)
            self.metagraph.save()
            bt.logging.info("Synced metagraph")
            return True
        except Exception as e:
            bt.logging.error(f"Error syncing metagraph: {e}")
            return False

    async def run_step(self):
        bt.logging.info("Starting run step")
        bt.logging.info("Attempting to sync metagraph")

        synced_metagraph = await self.try_sync_metagraph()
        if not synced_metagraph:
            bt.logging.warning("Failed to sync metagraph")
            return

        bt.logging.info("Getting current UIDs and hotkeys")
        current_uids = self.metagraph.uids.tolist()
        hotkeys = self.metagraph.hotkeys
        bt.logging.info(f"Current UIDs: {current_uids}")

        base_score = 1.0 / constants.NUM_UIDS
        for uid in current_uids:
            self.score_db.insert_or_reset_uid(uid, hotkeys[uid], base_score)

        bt.logging.info("Getting scores from database")
        db_scores = self.score_db.get_scores(current_uids)

        bt.logging.info("Setting weights tensor from database scores")
        self.weights = torch.tensor(db_scores, dtype=torch.float32)
        bt.logging.debug(f"Weights tensor: {self.weights}")

        self.consensus = self.metagraph.C
        bt.logging.debug(f"Consensus: {self.consensus}")

        bt.logging.info("Reading chain commitment")
        competition = read_chain_commitment(
            constants.SUBNET_OWNER, self.subtensor, self.config.netuid
        )
        if competition is None:
            bt.logging.error("Failed to read competition commitment")
            return

        eval_namespace = competition.repo

        bt.logging.info(f"Competition commitment: {competition}")

        bt.logging.info("Sampling competitors for evaluation")
        competitors = current_uids
        sample_size = min(self.config.miner_sample_size, len(competitors))
        uids_to_eval = self.rng.choice(competitors, sample_size, replace=False).tolist()
        lucky_num = int.from_bytes(os.urandom(4), "little")
        bt.logging.debug(f"UIDs to evaluate: {uids_to_eval}")

        scores_per_uid = {}
        block_per_uid = {}
        for uid in uids_to_eval:
            bt.logging.info(f"Evaluating UID: {uid}")
            bt.logging.info(
                f"Retrieving model metadata for hotkey: {self.metagraph.hotkeys[uid]}"
            )
            metadata = retrieve_model_metadata(
                self.subtensor, self.config.netuid, self.metagraph.hotkeys[uid]
            )
            if metadata is not None:
                bt.logging.info(f"Retrieved metadata: {metadata}")
                try:
                    miner_data_dir = os.path.join(self.config.data_dir, f"miner_{uid}")
                    eval_data_dir = self.config.eval_data_dir

                    bt.logging.info(f"Using data directory: {miner_data_dir}")
                    bt.logging.info(f"Using evaluation directory: {eval_data_dir}")

                    os.makedirs(miner_data_dir, exist_ok=True)
                    os.makedirs(eval_data_dir, exist_ok=True)

                    bt.logging.info(
                        f"Downloading training dataset: {metadata.id.namespace}/{metadata.id.commit}"
                    )

                    download_dataset(
                        metadata.id.namespace,
                        metadata.id.commit,
                        local_dir=miner_data_dir,
                        cache_dir=self.config.cache_dir,
                    )

                    bt.logging.info(
                        f"Downloading eval dataset: {eval_namespace}/{constants.eval_commit}"
                    )                   

                    download_dataset(
                        eval_namespace,
                        constants.eval_commit,
                        local_dir=eval_data_dir,
                        cache_dir=self.config.cache_dir,
                    )

                    bt.logging.info("Starting LoRA training")
                    eval_loss = train_lora(
                        lucky_num,
                        competition.bench,
                        cache_dir=self.config.cache_dir,
                        data_dir=miner_data_dir,
                        eval_data_dir=eval_data_dir,
                    )
                    bt.logging.info(f"Training complete with eval loss: {eval_loss}")

                    scores_per_uid[uid] = eval_loss
                    block_per_uid[uid] = metadata.block
                    bt.logging.info(f"Stored evaluation results for UID {uid}")

                except Exception as e:
                    bt.logging.error(f"train error: {e}")
                    scores_per_uid[uid] = 0

                finally:
                    bt.logging.info("Cleaning cache folder")
                    clean_cache_folder(miner_data_dir, eval_data_dir)
            else:
                bt.logging.warning(f"No metadata found for UID {uid}")
                scores_per_uid[uid] = 0

        duplicate_groups = []
        processed_uids = set()

        bt.logging.info("Checking for duplicate scores")
        for uid_i, score_i in scores_per_uid.items():
            # Skip UIDs with None or 0 scores, or already processed UIDs
            if score_i is None or score_i == 0 or uid_i in processed_uids:
                bt.logging.debug(
                    f"Skipping UID {uid_i} with score {score_i} (None, zero, or already processed)"
                )
                continue

            # Find all UIDs with nearly identical scores
            similar_uids = [uid_i]
            for uid_j, score_j in scores_per_uid.items():
                if (
                    uid_i != uid_j
                    and score_j is not None
                    and score_j != 0
                    and uid_j not in processed_uids
                ):
                    if math.isclose(score_i, score_j, rel_tol=1e-9):
                        bt.logging.debug(
                            f"Found similar score: {uid_i}({score_i}) and {uid_j}({score_j})"
                        )
                        similar_uids.append(uid_j)

            # If we found duplicates, add them to a group
            if len(similar_uids) > 1:
                bt.logging.info(f"Found duplicate group: {similar_uids}")
                duplicate_groups.append(similar_uids)
                processed_uids.update(similar_uids)

        duplicates = set()
        for group in duplicate_groups:
            bt.logging.info(f"Processing duplicate group: {group}")
            group.sort(key=lambda uid: block_per_uid[uid])
            bt.logging.info(f"Sorted by block: {group}")

            for uid in group[1:]:
                duplicates.add(uid)
                scores_per_uid[uid] = 0

        bt.logging.info("Normalizing scores")
        normalized_scores = {}
        for uid in uids_to_eval:
            if scores_per_uid[uid] is not None and scores_per_uid[uid] != 0:
                bt.logging.debug(
                    f"Computing normalized score for UID {uid} with raw score {scores_per_uid[uid]}"
                )
                if competition.bench is None or competition.bench <= 0:
                    bt.logging.warning(f"Invalid benchmark ({competition.bench}) for UID {uid}; defaulting score to 0")
                    normalized_score = 1.0 / constants.NUM_UIDS
                else:
                    normalized_score = compute_score(
                        scores_per_uid[uid], competition.bench
                    )
                normalized_scores[uid] = normalized_score
            else:
                bt.logging.debug(f"Setting zero normalized score for UID {uid}")
                normalized_scores[uid] = 0
        bt.logging.debug(f"Normalized scores: {normalized_scores}")

        bt.logging.info("Creating new weights tensor")
        new_weights = self.weights.clone()
        for uid, score in normalized_scores.items():
            new_weights[uid] = score

        new_weights = torch.where(
            new_weights < constants.MIN_WEIGHT_THRESHOLD,
            torch.zeros_like(new_weights),
            new_weights
        )
        bt.logging.debug(f"Thresholded weights (min {constants.MIN_WEIGHT_THRESHOLD}): {new_weights}")

        bt.logging.info("Updating database with score deltas")
        for uid in uids_to_eval:
            if uid < len(new_weights):
                final_weight = new_weights[uid].item()
                self.score_db.update_score(uid, final_weight)

        self.weights = new_weights
        bt.logging.debug(f"New weights: {new_weights}")
        bt.logging.debug(f"Consensus: {self.consensus}")

        bt.logging.info("Setting weights on chain")
        set_weights_with_err_msg(
            subtensor=self.subtensor,
            wallet=self.wallet,
            netuid=self.config.netuid,
            uids=self.metagraph.uids,
            weights=new_weights,
            wait_for_inclusion=True,
        )

    async def run(self):
        while True:
            await self.run_step()


if __name__ == "__main__":
    asyncio.run(Validator().run())
